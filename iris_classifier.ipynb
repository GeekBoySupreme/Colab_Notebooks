{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iris_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeekBoySupreme/Colab_Notebooks/blob/master/iris_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta3o2RYsuZa-",
        "colab_type": "text"
      },
      "source": [
        "# Iris Classification by eager execution\n",
        "This Notebook explores how to use machine learning to categorize Iris flowers by species. It uses TensorFlow's eager execution to (1) build a model, (2) train the model on example data, and (3) use the model to make predictions on unknown data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMg5vWBRupSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "add027fa-0c95-46cd-95c8-54b39d9833e5"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.14.0\n",
            "Eager execution: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59ulGO0uyZC",
        "colab_type": "text"
      },
      "source": [
        "#Import and parse the training dataset\n",
        "We need to download the dataset file and convert it to a structure that can be used by the Python program.\n",
        "\n",
        "#Download the dataset\n",
        "Download the training dataset file using the tf.keras.utils.get_file function. This returns the file path of the downloaded file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m1ZR95Qu4Ps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89f8666c-e577-4c28-cd41-6307be573a08"
      },
      "source": [
        "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
        "\n",
        "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
        "                                           origin=train_dataset_url)\n",
        "\n",
        "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Local copy of the dataset file: /root/.keras/datasets/iris_training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH-FE9sLu99L",
        "colab_type": "text"
      },
      "source": [
        "#Parse the dataset\n",
        "Since our dataset is a CSV-formatted text file, we'll parse the feature and label values into a format our Python model can use. Each line—or row—in the file is passed to the parse_csv function which grabs the first four feature fields and combines them into a single tensor. Then, the last field is parsed as the label. The function returns both the features and label tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYyq-7yvvBRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_csv(line):\n",
        "  example_defaults = [[0.], [0.], [0.], [0.], [0]]  # sets field types\n",
        "  parsed_line = tf.decode_csv(line, example_defaults)\n",
        "  # First 4 fields are features, combine into single tensor\n",
        "  features = tf.reshape(parsed_line[:-1], shape=(4,))\n",
        "  # Last field is the label\n",
        "  label = tf.reshape(parsed_line[-1], shape=())\n",
        "  return features, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_12q_xCvSfT",
        "colab_type": "text"
      },
      "source": [
        "**Create the training tf.data.Dataset**\n",
        "\n",
        "This program uses tf.data.TextLineDataset to load a CSV-formatted text file and is parsed with our *parse_csv* function. A *tf.data.Dataset *represents an input pipeline as a collection of elements and a series of transformations that act on those elements. \n",
        "\n",
        "Transformation methods are chained together or called sequentially—just make sure to keep a reference to the returned Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVD5uwNgvgJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "00c4e79a-9ecf-483f-e5c9-7c1df6e1875a"
      },
      "source": [
        "train_dataset = tf.data.TextLineDataset(train_dataset_fp)\n",
        "train_dataset = train_dataset.skip(1)             # skip the first header row\n",
        "train_dataset = train_dataset.map(parse_csv)      # parse each row\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1000)  # randomize\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "# View a single example entry from a batch\n",
        "features, label = iter(train_dataset).next()\n",
        "print(\"example features:\", features[0])\n",
        "print(\"example label:\", label[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example features: tf.Tensor([5.1 3.5 1.4 0.3], shape=(4,), dtype=float32)\n",
            "example label: tf.Tensor(0, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-qKQ764vrti",
        "colab_type": "text"
      },
      "source": [
        "**Create a model using Keras**\n",
        "\n",
        "The TensorFlow tf.keras API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDmt8dGBvuyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(4,)),  # input shape required\n",
        "  tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(3)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_qnoIHUv2JD",
        "colab_type": "text"
      },
      "source": [
        "#Train the Model\n",
        "**Define the loss and gradient function**\n",
        "\n",
        "Both training and evaluation stages need to calculate the model's *loss*. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n",
        "\n",
        "Our model will calculate its loss using the `tf.losses.sparse_softmax_cross_entropy` function which takes the model's prediction and the desired label. The returned loss value is progressively larger as the prediction gets worse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6qQMmyAv_Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(model, x, y):\n",
        "  y_ = model(x)\n",
        "  return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
        "\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss_value = loss(model, inputs, targets)\n",
        "  return tape.gradient(loss_value, model.variables)\n",
        "\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wITi_U8-wLAT",
        "colab_type": "text"
      },
      "source": [
        "**Training loop**\n",
        "\n",
        "With all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n",
        "\n",
        "Iterate each epoch. An epoch is one pass through the dataset.\n",
        "Within an epoch, iterate over each example in the training Dataset grabbing its features (x) and label (y).\n",
        "Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n",
        "Use an optimizer to update the model's variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JoRfjlzwOvC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "793637b4-e4e1-4989-afe0-949da3659c27"
      },
      "source": [
        "## Note: Rerunning this cell uses the same model variables\n",
        "\n",
        "# keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "\n",
        "num_epochs = 201\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss_avg = tfe.metrics.Mean()\n",
        "  epoch_accuracy = tfe.metrics.Accuracy()\n",
        "\n",
        "  # Training loop - using batches of 32\n",
        "  for x, y in train_dataset:\n",
        "    # Optimize the model\n",
        "    grads = grad(model, x, y)\n",
        "    optimizer.apply_gradients(zip(grads, model.variables),\n",
        "                              global_step=tf.train.get_or_create_global_step())\n",
        "\n",
        "    # Track progress\n",
        "    epoch_loss_avg(loss(model, x, y))  # add current batch loss\n",
        "    # compare predicted label to actual label\n",
        "    epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y)\n",
        "\n",
        "  # end epoch\n",
        "  train_loss_results.append(epoch_loss_avg.result())\n",
        "  train_accuracy_results.append(epoch_accuracy.result())\n",
        "  \n",
        "  if epoch % 50 == 0:\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 000: Loss: 1.793, Accuracy: 35.000%\n",
            "Epoch 050: Loss: 0.694, Accuracy: 69.167%\n",
            "Epoch 100: Loss: 0.395, Accuracy: 94.167%\n",
            "Epoch 150: Loss: 0.257, Accuracy: 96.667%\n",
            "Epoch 200: Loss: 0.172, Accuracy: 98.333%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWULUam-wZ3s",
        "colab_type": "text"
      },
      "source": [
        "**Setup the test dataset**\n",
        "\n",
        "Evaluating the model is similar to training the model. The biggest difference is the examples come from a separate *test set* rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vRh7qU5wcg8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c2f24d9c-c092-4906-9194-a2795bbe8044"
      },
      "source": [
        "test_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
        "\n",
        "test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),\n",
        "                                  origin=test_url)\n",
        "\n",
        "test_dataset = tf.data.TextLineDataset(test_fp)\n",
        "test_dataset = test_dataset.skip(1)             # skip header row\n",
        "test_dataset = test_dataset.map(parse_csv)      # parse each row with the funcition created earlier\n",
        "test_dataset = test_dataset.shuffle(1000)       # randomize\n",
        "test_dataset = test_dataset.batch(32)           # use the same batch size as the training set"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/iris_test.csv\n",
            "\r8192/573 [============================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwmdQDSawiTj",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate the model on the test dataset**\n",
        "\n",
        "Unlike the training stage, the model only evaluates a single epoch of the test data. In the following code cell, we iterate over each example in the test set and compare the model's prediction against the actual label. This is used to measure the model's accuracy across the entire test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdGNjOUwkub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6eb08f67-9cf6-4360-c3ed-d2661877a9c4"
      },
      "source": [
        "test_accuracy = tfe.metrics.Accuracy()\n",
        "\n",
        "for (x, y) in test_dataset:\n",
        "  prediction = tf.argmax(model(x), axis=1, output_type=tf.int32)\n",
        "  test_accuracy(prediction, y)\n",
        "\n",
        "print(\"Test set accuracy: {:.4%}\".format(test_accuracy.result()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy: 93.3333%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJSUDpJSwuAb",
        "colab_type": "text"
      },
      "source": [
        "#Use the trained model to make predictions\n",
        "\n",
        "We've trained a model and \"proven\" that it's good—but not perfect—at classifying Iris species. Now let's use the trained model to make some predictions on unlabeled examples; that is, on examples that contain features but not a label.\n",
        "\n",
        "In real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For now, we're going to manually provide three unlabeled examples to predict their labels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5Ze8NqXwxt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddd3c1af-95f0-498c-c20e-45ad7f322767"
      },
      "source": [
        "class_ids = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"]\n",
        "\n",
        "predict_dataset = tf.convert_to_tensor([\n",
        "    [5.1, 3.3, 1.7, 0.5,],\n",
        "    [5.9, 3.0, 4.2, 1.5,],\n",
        "    [6.9, 3.1, 5.4, 2.1]\n",
        "])\n",
        "\n",
        "predictions = model(predict_dataset)\n",
        "\n",
        "for i, logits in enumerate(predictions):\n",
        "  class_idx = tf.argmax(logits).numpy()\n",
        "  name = class_ids[class_idx]\n",
        "  print(\"Example {} prediction: {}\".format(i, name))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example 0 prediction: Iris setosa\n",
            "Example 1 prediction: Iris versicolor\n",
            "Example 2 prediction: Iris virginica\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}